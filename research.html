---
layout: page
title: Research
---

<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <style>
        .table {
            width: 100%;
            border-collapse: collapse;
        }
        .table td {
            padding: 8px;
            border-right: none;
            border-left: none;
        }
        .heading-block {
            border: 1px solid #ddd;
            background-color: #a6a6a6;
            padding: 10px;
            margin-bottom: 10px;
        }

    </style>
</head>


<body>

<div class="heading-block">
    <h3>Research Students</h3>
</div>

<table class="table">
    <tr>
        <td><strong>Shaheer Afridi</strong></td>
        <td>"Medical Imaging Segmentation through State-of-the-art Deep Learning Technique using 3D volumetric Data"</td>
    </tr>
    <tr>
        <td><strong>Utsab Chalise</strong></td>
        <td>"Generating Brain MRI Images for Spatial and Volumetric Upsampling Using Diffusion Techniques"</td>
    </tr>
    <tr>
        <td><strong>Akintade Egbetakin</strong></td>
        <td>"Computer Vision and Deep Learning for the Detection of COVID-19, Pneumonia, and Tuberculosis using Chest X-ray Images"</td>
    </tr>
    <tr>
        <td><strong>Efoma Ibude</strong></td>
        <td>"Satellite Image Super Resolution Using GAN Techniques"</td>
    </tr>
</table>

<br>



<div class="heading-block">
    <h3>Research Collaborators</h3>
</div>

<table class="table">
    <tr>
        <td>
            <strong><a href="https://www.brookes.ac.uk/profiles/staff/fabio-cuzzolin">
            Prof. Fabio Cuzzolin</a></strong><br>
            Oxford Brookes University, UK
        </td>

        <td>
            <strong><a href="https://profiles.imperial.ac.uk/george.mylonas">
            Dr George Mylonas</a></strong><br>
            imperial College London, UK
        </td>
    </tr>
    <tr>
        <td>
            <strong><a href="https://www.linkedin.com/in/shailza-sharma-21a59a220">
            Dr Shailza Sharma</a></strong><br>
            University of Leeds, UK
        </td>
    </tr>

</table>


<br>


<div class="heading-block">
    <h3>Datasets</h3>
</div>

<table border="0" cellspacing="0" cellpadding="0" style="width: 100%;">
    <tr>
        <td style="width: 25%;">
            <figure>
                <figcaption>
                    <a href="https://saras-esad.grand-challenge.org/download/">SARAS-ESAD Dataset</a>
                </figcaption><br><br>
                <img src="/assets/img/paper/esad_dataset.png" align="top" style="width: 100%;">
            </figure>
        </td>
        <td style="width: 75%;">
            The SARAS-ESAD is a surgical action detection dataset developed to build AI systems to assist surgeons 
            during minimally invasive surgery. The system needs to detect and understand the actions performed by 
            the surgeons in endoscopic videos in real-time. The datasets provide action information and locations 
            annotations for surgical tools on recoded videos during real surgeries. The dataset contains 21 action 
            classes and each frame is annotated for all the actions being performed in that instance. The dataset 
            is divided into training, validation, and test sets. The training data contains over 22,000 frames 
            with annotations for over 28,000 actions. The dataset was released in SARAS-ESAD challenge and can 
            be downloaded from the challenge website. 
        </td>
    </tr>
    <tr>
        <td style="width: 25%;">
            <figure>
                <figcaption>
                    <a href="https://saras-mesad.grand-challenge.org/download/">SARAS-MESAD Dataset</a>
                </figcaption><br><br>
                <img src="/assets/img/paper/mesad_dataset.png" align="top" style="width: 100%;">
            </figure>
        </td>
        <td style="width: 75%;">
            The SARAS challenge expands the SARAS-ESAD dataset for surgeon action detection in 
            Robotic-Assisted Radical Prostatectomy (RARP) surgery. It introduces the MESAD dataset 
            with two parts: MESAD-Real capturing real-world RARP surgeries on patients, and MESAD-Phantom 
            using videos of procedures on training phantoms designed for the challenge. Both datasets aim 
            to improve AI systems' ability to detect surgeon actions during RARP surgery. 
        </td>
    </tr>
</table>


<br>



<div class="heading-block">
    <h3>Softwares</h3>
</div>


<table border="0" cellspacing="0" cellpadding="0" style="width: 100%;">
    <tr>
        <td style="width: 100%; padding: 5px;">
            <div style="float: right; margin-right: 10px;">
                <iframe src="https://drive.google.com/file/d/1Ey32z2bgyEDJ0YEFsLnscKr91nQE6dvB/preview" 
                    width="320" height="200" allow="autoplay"></iframe>
            </div>
            <strong><a href="https://github.com/Viveksbawa/SARAS-ESAD-Baseline">
                Surgical Action Detection</a></strong><br>
            Write your description about the video content here. You can add multiple paragraphs, 
            headings, or other elements to structure your details. Write your description about the video content here. You can add multiple paragraphs, 
            headings, or other elements to structure your details. Write your description about the video content here. You can add multiple paragraphs, 
            headings, or other elements to structure your details. Write your description about the video content here. You can add multiple paragraphs, 
            headings, or other elements to structure your details. Write your description about the video content here. You can add multiple paragraphs, 
            headings, or other elements to structure your details.
        </td>
    </tr>
    <tr>
        <td style="width: 100%; padding: 5px;">
            <div style="float: right; margin-right: 10px;">
                <iframe src="https://drive.google.com/file/d/1PcopBI2y7mRR7Yi149TJ_R9op172UmpW/preview" 
                width="320" height="200" allow="autoplay"></iframe>
            </div>
            <strong><a href="https://github.com/Viveksbawa/semantic_SARAS">
                Surgical tools & organs segmentation</a></strong><br>
                Semantic segmentation of surgical scene is very complex and difficult task. As we are dealing 
                with deformable organs. Additionally, view point is very close as there is not much distance 
                between endoscope camera and the organs. minor movement of the endoscope creates vast changes 
                in structure of scene. We have developed a dataset for semantic scene understanding of 
                surgical scene.
        </td>
    </tr>
</table>


<br>




<div class="heading-block">
    <h3>Key Publications</h3>
</div>

<table border="0" cellspacing="0" cellpadding="0" style="width: 100%;">

    <tr>
        <td style="width: 25%;">
            <img src="/assets/img/paper/dino.png" align="top" style="width: 100%;">
        </td>
        <td style="width: 75%;">
            <b> Temporal DINO: A Self-supervised Video Strategy to Enhance Action Prediction</b> <br>
            <i>Izzeddin Teeti, Rongali Sai Bhargav, Vivek Singh, Andrew Bradley, Biplab Banerjee, Fabio Cuzzolin</i> <br>
            <i> IEEE/CVF International Conference on Computer Vision Workshops (ICCVW), 2023 </i> <br>
            [<a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10350489">link</a>]
        </td>
    </tr>

    <tr>
        <td style="width: 25%;">
            <img src="/assets/img/paper/superresolution.png" align="top" style="width: 100%;">
        </td>
        <td style="width: 75%;">
            <b>Dual Stage Semantic Information Based Generative Adversarial Network For Image Super-Resolution</b> <br>
            <i>Shailza Sharma, Abhinav Dhall, Dr Vinay Kumar, Vivek Singh</i> <br>
            <i> Fourteenth Indian Conference on Computer Vision, Graphics and Image Processing (ICVGIP), 2023</i> <br>
            [<a href="https://dl.acm.org/doi/abs/10.1145/3627631.3627646">link</a>]
        </td>
    </tr>

    <tr>
        <td style="width: 25%;">
            <img src="/assets/img/paper/esad.png" align="top" style="width: 100%;">
        </td>
        <td style="width: 75%;">
            <b> The saras endoscopic surgeon action detection (esad) dataset: Challenges and methods</b> <br>
            <i> Vivek Singh Bawa, Gurkirt Singh, Francis KapingA, Inna Skarga-Bandurova, Elettra Oleari, 
                Alice Leporini, Carmela Landolfo, Pengfei Zhao, Xi Xiang, Gongning Luo, Kuanquan Wang, 
                Liangzhi Li, Bowen Wang, Shang Zhao, Li Li, Armando Stabile, Francesco Setti, 
                Riccardo Muradore, Fabio Cuzzolin</i> <br>
            <i> ArXiv, 2021 </i> <br>
            [<a href="https://arxiv.org/pdf/2104.03178">pdf</a>]
        </td>
    </tr>

    <tr>
        <td style="width: 25%;">
            <img src="/assets/img/paper/facial.png" align="top" style="width: 100%;">
        </td>
        <td style="width: 75%;">
            <b>An automatic multimedia likability prediction system based on facial expression of observer</b> <br>
            <i>Vivek Singh Bawa, Shailza Sharma, Mohammed Usman, Abhimat Gupta, Vinay Kumar</i> <br>
            <i>IEEE Access, 2021 </i> <br>
            [<a href="https://ieeexplore.ieee.org/abstract/document/9504548">link</a>] 
        </td>
    </tr>

    <tr>
        <td style="width: 25%;">
            <img src="/assets/img/paper/lisa.png" align="top" style="width: 100%;">
        </td>
        <td style="width: 75%;">
            <b> Linearized sigmoidal activation: A novel activation function with tractable non-linear 
                characteristics to boost representation capability</b> <br>
            <i>  Vivek Singh Bawa and Vinay Kumar </i> <br>
            <i> Expert Systems with Applications, 2019 </i> <br>
            [<a href="https://www.sciencedirect.com/science/article/pii/S0957417418307619">link</a>]
        </td>
    </tr>
</table>

</body>

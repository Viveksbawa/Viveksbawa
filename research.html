---
layout: page
title: Research
---

<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <style>
        .table {
            width: 100%;
            border-collapse: collapse;
        }
        .table td {
            padding: 8px;
            border-right: none;
            border-left: none;
        }
        .heading-block {
            border: 1px solid #ddd;
            background-color: #a6a6a6;
            padding: 5px;
            margin-bottom: 10px;
        }

    </style>
</head>


<body>

<div class="heading-block">
    <h3>Students</h3>
</div>

<table class="table">
    <tr>
        <td><strong>Shaheer Afridi</strong></td>
        <td>"Medical Imaging Segmentation through State-of-the-art Deep Learning Technique using 3D volumetric Data"</td>
    </tr>
    <tr>
        <td><strong>Utsab Chalise</strong></td>
        <td>"Generating Brain MRI Images for Spatial and Volumetric Upsampling Using Diffusion Techniques"</td>
    </tr>
    <tr>
        <td><strong>Akintade Egbetakin</strong></td>
        <td>"Computer Vision and Deep Learning for the Detection of COVID-19, Pneumonia, and Tuberculosis using Chest X-ray Images"</td>
    </tr>
    <tr>
        <td><strong>Efoma Ibude</strong></td>
        <td>"Satellite Image Super Resolution Using GAN Techniques"</td>
    </tr>
</table>


<br>


<div class="heading-block">
    <h3>Collaborators</h3>
</div>

<table class="table">
    <tr>
        <td>
            <strong><a href="https://www.brookes.ac.uk/profiles/staff/fabio-cuzzolin">
            Prof Fabio Cuzzolin</a></strong><br>
            Oxford Brookes University, UK
        </td>

        <td>
            <strong><a href="https://profiles.imperial.ac.uk/george.mylonas">
            Dr George Mylonas</a></strong><br>
            Imperial College London, UK
        </td>
    </tr>
    <tr>
        <td>
            <strong><a href="https://www.linkedin.com/in/shailza-sharma-21a59a220">
            Dr Shailza Sharma</a></strong><br>
            University of Leeds, UK
        </td>
    </tr>

</table>


<br>


<div class="heading-block">
    <h3>Datasets</h3>
</div>

<table border="0" cellspacing="0" cellpadding="0" style="width: 100%;">
    <tr>
        <td style="width: 25%;">
            <figure>
                <figcaption>
                    <a href="https://saras-esad.grand-challenge.org/download/">SARAS-ESAD Dataset</a>
                </figcaption><br><br>
                <img src="/assets/img/paper/esad_dataset.png" align="top" style="width: 100%;">
            </figure>
        </td>
        <td style="width: 75%;">
            The SARAS-ESAD dataset is a surgical action detection dataset created to develop AI systems to 
            assist surgeons during minimally invasive surgery, specifically Robotic-Assisted Radical 
            Prostatectomy (RARP) procedures. The dataset is designed to recognize and localize the actions 
            performed by surgeons in RARP videos. It contains 21 action classes. Each annotated frame has 
            one or more action instances. The dataset is divided into training, validation, and test sets. 
            The training data contains over 22,000 frames with annotations for over 28,000 actions. The dataset 
            was released for the SARAS-ESAD challenge and can be downloaded from the challenge 
            <a href="https://saras-esad.grand-challenge.org/">website</a>.
            
        </td>
    </tr>
    <tr>
        <td style="width: 25%;">
            <figure>
                <figcaption>
                    <a href="https://saras-mesad.grand-challenge.org/download/">SARAS-MESAD Dataset</a>
                </figcaption><br><br>
                <img src="/assets/img/paper/mesad_dataset.png" align="top" style="width: 100%;">
            </figure>
        </td>
        <td style="width: 75%;">
            The SARAS-MESAD dataset expands the SARAS-ESAD dataset for surgeon action detection in 
            Robotic-Assisted Radical Prostatectomy (RARP) surgery. MESAD consists of two subsets: MESAD-Real 
            and MESAD-Phantom. MESAD-Real contains videos of real RARP surgeries on patients, while MESAD-Phantom
             includes videos of procedures performed on training phantoms with anatomically similar structures. 
             The main objective of this dataset is to explore the possibility of using artificial phantoms to 
             improve the performance of surgeon action detection on real patient data. For more details and to 
             download the dataset, please visit the challenge 
             <a href="https://saras-mesad.grand-challenge.org/">website</a>. 
        </td>
    </tr>
</table>


<br>


<div class="heading-block">
    <h3>Softwares</h3>
</div>


<table border="0" cellspacing="0" cellpadding="0" style="width: 100%;">
    <tr>
        <td style="width: 100%; padding: 5px;">
            <div style="float: right; margin-right: 10px;">
                <iframe src="https://drive.google.com/file/d/1Ey32z2bgyEDJ0YEFsLnscKr91nQE6dvB/preview" 
                    width="320" height="190" allow="autoplay"></iframe>
            </div>
            <strong><a href="https://github.com/Viveksbawa/SARAS-ESAD-Baseline">
                Surgical Action Detection</a></strong><br>
                This code implements a surgeon action detection model using a Feature Pyramid Network (FPN) 
                architecture. The FPN leverages a convolutional neural network (CNN) with pooling layers to 
                create a rich hierarchy of features for accurate action recognition. A powerful ResNet 
                serves as the model's backbone, extracting features from various stages within the surgical 
                video. These multi-level features are then processed to predict both the type of action 
                (class label) and the bounding box location for the detected surgeon actions. To prevent 
                overfitting and improve generalization, we employ a range of data augmentation techniques 
                alongside freezing specific layers during training.
        </td>
    </tr>
    <tr>
        <td style="width: 100%; padding: 5px;">
            <div style="float: right; margin-right: 10px;">
                <iframe src="https://drive.google.com/file/d/1PcopBI2y7mRR7Yi149TJ_R9op172UmpW/preview" 
                width="320" height="200" allow="autoplay"></iframe>
            </div>
            <strong><a href="https://github.com/Viveksbawa/semantic_SARAS">
                Surgical Tools & Organs Segmentation</a></strong><br>
                Semantic segmentation of surgical scenes is a very complex and challenging task. Two key 
                factors contribute to this difficulty: the deformable nature of organs and the close viewpoint 
                of the endoscope camera. Organs can change shape significantly throughout surgery, and the 
                limited field of view from the close-up camera means even minor movements of the endoscope 
                can drastically alter the scene's structure. To address these challenges, we have developed 
                a segmentation model specifically for semantic understanding of surgical scenes.
        </td>
    </tr>
</table>


<br>


<div class="heading-block">
    <h3>Selected Publications</h3>
</div>

<table border="0" cellspacing="0" cellpadding="0" style="width: 100%;">

    <tr>
        <td style="width: 25%;">
            <img src="/assets/img/paper/dino.png" align="top" style="width: 100%;">
        </td>
        <td style="width: 75%;">
            <b> Temporal DINO: A Self-supervised Video Strategy to Enhance Action Prediction</b> <br>
            <i>Izzeddin Teeti, Rongali Sai Bhargav, Vivek Singh, Andrew Bradley, Biplab Banerjee, Fabio Cuzzolin</i> <br>
            <i> IEEE/CVF International Conference on Computer Vision Workshops (ICCVW), 2023 </i> <br>
            [<a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10350489">link</a>]
        </td>
    </tr>

    <tr>
        <td style="width: 25%;">
            <img src="/assets/img/paper/superresolution.png" align="top" style="width: 100%;">
        </td>
        <td style="width: 75%;">
            <b>Dual Stage Semantic Information Based Generative Adversarial Network For Image Super-Resolution</b> <br>
            <i>Shailza Sharma, Abhinav Dhall, Dr Vinay Kumar, Vivek Singh</i> <br>
            <i> Fourteenth Indian Conference on Computer Vision, Graphics and Image Processing (ICVGIP), 2023</i> <br>
            [<a href="https://dl.acm.org/doi/abs/10.1145/3627631.3627646">link</a>]
        </td>
    </tr>

    <tr>
        <td style="width: 25%;">
            <img src="/assets/img/paper/esad.png" align="top" style="width: 100%;">
        </td>
        <td style="width: 75%;">
            <b> The saras endoscopic surgeon action detection (esad) dataset: Challenges and methods</b> <br>
            <i> Vivek Singh Bawa, Gurkirt Singh, Francis KapingA, Inna Skarga-Bandurova, Elettra Oleari, 
                Alice Leporini, Carmela Landolfo, Pengfei Zhao, Xi Xiang, Gongning Luo, Kuanquan Wang, 
                Liangzhi Li, Bowen Wang, Shang Zhao, Li Li, Armando Stabile, Francesco Setti, 
                Riccardo Muradore, Fabio Cuzzolin</i> <br>
            <i> ArXiv, 2021 </i> <br>
            [<a href="https://arxiv.org/pdf/2104.03178">pdf</a>]
        </td>
    </tr>

    <tr>
        <td style="width: 25%;">
            <img src="/assets/img/paper/facial.png" align="top" style="width: 100%;">
        </td>
        <td style="width: 75%;">
            <b>An automatic multimedia likability prediction system based on facial expression of observer</b> <br>
            <i>Vivek Singh Bawa, Shailza Sharma, Mohammed Usman, Abhimat Gupta, Vinay Kumar</i> <br>
            <i>IEEE Access, 2021 </i> <br>
            [<a href="https://ieeexplore.ieee.org/abstract/document/9504548">link</a>] 
        </td>
    </tr>

    <tr>
        <td style="width: 25%;">
            <img src="/assets/img/paper/lisa.png" align="top" style="width: 100%;">
        </td>
        <td style="width: 75%;">
            <b> Linearized sigmoidal activation: A novel activation function with tractable non-linear 
                characteristics to boost representation capability</b> <br>
            <i>  Vivek Singh Bawa and Vinay Kumar </i> <br>
            <i> Expert Systems with Applications, 2019 </i> <br>
            [<a href="https://www.sciencedirect.com/science/article/pii/S0957417418307619">link</a>]
        </td>
    </tr>
</table>

</body>
